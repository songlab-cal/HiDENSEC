{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc030afe-49da-424c-90d6-47144ae09ac2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Global Variables & Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b82194-db28-4031-a8dd-a9933076225e",
   "metadata": {},
   "source": [
    "These global definitions require evaluation before running HiDENSEC on any concrete Hi-C map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac05b69-b36b-4020-a567-3f487f63b9e3",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49eafd0-f53b-4325-bae7-a82d62b67006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp_sparse\n",
    "import scipy.signal as sp_signal\n",
    "import scipy.ndimage as sp_image\n",
    "from scipy.stats import kendalltau\n",
    "import scipy.stats as sp_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646753a7-89f8-4748-95fa-8ef999fc7232",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5674ea00-47cd-4a52-b669-09e8d6f9d8ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a057115-3df8-4f99-9e9f-00bf3269613a",
   "metadata": {},
   "source": [
    "Load file paths, covariates & centromere locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "2873acd8-9e34-44bd-8db7-656c9bf11276",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosomelocations = np.loadtxt('chromosomelocations.txt')-1\n",
    "compartmentnames = np.loadtxt('compartmentnames.txt', dtype='str')\n",
    "compartments = np.loadtxt('compartments.txt', dtype='str')\n",
    "centromers = np.loadtxt('centromers.txt')-1\n",
    "newCentromers = np.loadtxt('newCentromers.txt')-1\n",
    "rPos = np.loadtxt('rPos.txt')-1\n",
    "fPos = np.loadtxt('fPos.txt')-1\n",
    "acrox = np.loadtxt('acrox.txt')\n",
    "corrchromlocations = np.loadtxt('corrchromlocations.txt')-1\n",
    "centromersCorrected = np.loadtxt('centromersCorrected.txt')-1\n",
    "covariates = np.loadtxt('covariates.csv', delimiter=',')\n",
    "excursionLengthsH0 = np.loadtxt('excursionLengthsH0.csv', delimiter=',')\n",
    "fixCdata = np.loadtxt('fixCdata.csv', delimiter=',', dtype='object')\n",
    "hiCdata = np.loadtxt('hiCdata.csv', delimiter=',', dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f575a37-cd94-4bed-ae3b-de85c814d02c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Covariate Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6286632f-d305-4fa9-85d8-96fd556d5e04",
   "metadata": {},
   "source": [
    "Function definitions pertaining to covariate regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "9389ba95-fd6b-47e9-adab-897458d15f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partitionByIndex(list, indices):\n",
    "    return np.split(list, indices)\n",
    "\n",
    "def partitionByLength(list, lengths):\n",
    "    return partitionByIndex(list, np.cumsum(lengths))\n",
    "\n",
    "def findDiploidPeak(data, window=[0.1,1,0.1]):\n",
    "    clean_data = data[data>0]\n",
    "    scale = np.median(np.abs(clean_data - np.median(clean_data)))\n",
    "    modes = []\n",
    "    for c in np.arange(window[0], window[1] + window[2], window[2]):\n",
    "        counts, bins = np.histogram(clean_data, bins=np.arange(clean_data.min(), clean_data.max(), c*scale))\n",
    "        mode_index = int(np.median(np.argmax(counts)))\n",
    "        modes.append((bins[mode_index] + bins[mode_index+1])/2)\n",
    "    return np.mean(modes)\n",
    "\n",
    "def filterPosition(dataset, cutthreshold=75, GCthreshold=0.32, mapthreshold=0.8):\n",
    "    covariate_part = dataset[:, 1:4].astype(float)\n",
    "    compartment_part = dataset[:,4]\n",
    "    return (covariate_part[:,0]>cutthreshold) & (covariate_part[:,1]>GCthreshold) & (covariate_part[:,2]>mapthreshold) & (compartment_part!='A0')\n",
    "\n",
    "def covariateFilter(dataset, cutthreshold=75, GCthreshold=0.32, mapthreshold=0.8):\n",
    "    return dataset[filterPosition(dataset, cutthreshold, GCthreshold, mapthreshold)]\n",
    "\n",
    "def covariateCorrection1(dataset, cutthreshold=75, GCthreshold=0.32, mapthreshold=0.8, midpointwindow=[0.1,1,0.1], neighbourhood=0.3, filterlength=100):\n",
    "    data = covariateFilter(dataset, cutthreshold, GCthreshold, mapthreshold)\n",
    "    midpoints = [ findDiploidPeak(data[data[:,4] == compartment_name, 0].astype(float), midpointwindow) for compartment_name in compartmentnames[1:] ]\n",
    "    inserted_midpoints = np.copy(data[:,-1])\n",
    "    for j in range(5):\n",
    "        inserted_midpoints[inserted_midpoints == compartmentnames[1+j]] = midpoints[j]\n",
    "    compartmentcorrected = 2*data[:,0].astype(float)/inserted_midpoints.astype(float)\n",
    "    predictions = np.copy(data[:, -1])\n",
    "    for compartment_name in compartmentnames[1:]:\n",
    "        compartment_indices = (data[:, -1] == compartment_name) & (np.abs(compartmentcorrected - 2) < neighbourhood)\n",
    "        compartment_covariates = data[compartment_indices][:, [1,2]].astype(float)\n",
    "        compartment_values = compartmentcorrected[compartment_indices].astype(float)\n",
    "        design_mat = np.concatenate( [np.ones([compartment_indices.sum(),1]), compartment_covariates], axis=1 )\n",
    "        fit = np.linalg.lstsq(design_mat, compartment_values, rcond=None)\n",
    "        full_compartment_indices = (data[:, -1] == compartment_name)\n",
    "        full_compartment_covariates = data[full_compartment_indices][:, [1,2]].astype(float)\n",
    "        full_design_mat = np.concatenate( [np.ones([full_compartment_indices.sum(),1]), full_compartment_covariates], axis=1 )\n",
    "        predictions[predictions == compartment_name] = full_design_mat @ fit[0]\n",
    "    fully_corrected_data = 2*compartmentcorrected / predictions.astype(float)\n",
    "    fully_corrected_data = 2*fully_corrected_data / findDiploidPeak(fully_corrected_data)\n",
    "    return fully_corrected_data\n",
    "\n",
    "def attachCovariates(profile):\n",
    "    return np.concatenate([profile[:,None], np.transpose(covariates), compartments[:,None]], axis=1)\n",
    "\n",
    "def rawToCorrected(x, filter):\n",
    "    if type(x) == int:\n",
    "        return int(np.median(np.argmin(np.abs(x-filter))))\n",
    "    else:\n",
    "        return [int(np.median(np.argmin(np.abs(y-filter)))) for y in x]\n",
    "\n",
    "def xToChi(x):\n",
    "    return np.argmax(x < corrchromlocations[1:])+1\n",
    "\n",
    "def dataCcorrector(profile, data_flag, cutthreshold=75, GCthreshold=0.32, mapthreshold=0.8, midpointwindow=[0.1,1,0.1], neighbourhood=0.3, filterlength=100):\n",
    "    test_data = attachCovariates(profile)\n",
    "    test_data = covariateFilter(test_data, cutthreshold, GCthreshold, mapthreshold)\n",
    "    if data_flag == 'HiC':\n",
    "        reference_data = np.copy(hiCdata)\n",
    "    elif data_flag == 'FixC':\n",
    "        reference_data = np.copy(fixCdata)\n",
    "    else:\n",
    "        return 'Unknown protocol'\n",
    "    reference_data = reference_data[:, [4, 1, 2, 3, 0]]\n",
    "    data = covariateFilter(reference_data, cutthreshold, GCthreshold, mapthreshold)\n",
    "    midpoints = [ findDiploidPeak(data[data[:,4] == ('\"'+compartment_name+'\"'), 0].astype(float), midpointwindow) for compartment_name in compartmentnames[1:] ]\n",
    "    inserted_midpoints = np.copy(data[:,-1])\n",
    "    for j in range(5):\n",
    "        inserted_midpoints[inserted_midpoints == ('\"'+compartmentnames[1+j]+'\"')] = midpoints[j]\n",
    "    compartmentcorrected = 2*data[:,0].astype(float)/inserted_midpoints.astype(float)\n",
    "    test_inserted_midpoints = np.copy(test_data[:,-1])\n",
    "    for j in range(5):\n",
    "        test_inserted_midpoints[test_inserted_midpoints == (compartmentnames[1+j])] = midpoints[j]\n",
    "    test_compartmentcorrected = 2*test_data[:,0].astype(float)/test_inserted_midpoints.astype(float)\n",
    "    predictions = np.copy(test_data[:, -1])\n",
    "    for compartment_name in compartmentnames[1:]:\n",
    "        compartment_indices = (data[:, -1] == ('\"'+compartment_name+'\"')) & (np.abs(compartmentcorrected - 2) < neighbourhood)\n",
    "        compartment_covariates = data[compartment_indices][:, [1,2]].astype(float)\n",
    "        compartment_values = compartmentcorrected[compartment_indices].astype(float)\n",
    "        design_mat = np.concatenate( [np.ones([compartment_indices.sum(),1]), compartment_covariates], axis=1 )\n",
    "        fit = np.linalg.lstsq(design_mat, compartment_values, rcond=None)\n",
    "        full_compartment_indices = (test_data[:, -1] == compartment_name)\n",
    "        full_compartment_covariates = test_data[full_compartment_indices][:, [1,2]].astype(float)\n",
    "        full_design_mat = np.concatenate( [np.ones([full_compartment_indices.sum(),1]), full_compartment_covariates], axis=1 )\n",
    "        predictions[predictions == compartment_name] = full_design_mat @ fit[0]\n",
    "    fully_corrected_data = 2*test_compartmentcorrected / predictions.astype(float)\n",
    "    fully_corrected_data = 2*medianFilter(fully_corrected_data, 100) / findDiploidPeak(medianFilter(fully_corrected_data, 100))\n",
    "    return fully_corrected_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56889ff-0b67-4b6a-a318-af35abe8e93a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Copy number & Mixture Proportion Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4235d86e-f3a4-4f45-a509-12e13bdec8c3",
   "metadata": {},
   "source": [
    "Function definitions calculating effective copy number profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "93805730-357f-405b-8513-81ff1bec8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def medianDeviation(list):\n",
    "    return np.median(np.abs(list - np.median(list)))\n",
    "\n",
    "def splitList(list, index):\n",
    "    return ([list[:index], list[index:]], [index / len(list), 1-index/len(list)])\n",
    "\n",
    "def findRestrictedMedian(list, candidates):\n",
    "    epsilon_list = [np.mean(np.abs(list - candidate)) for candidate in candidates]\n",
    "    minPos = np.argmin(epsilon_list)\n",
    "    return (candidates[minPos], epsilon_list[minPos], medianDeviation(list))\n",
    "\n",
    "def findSplit(data, candidates):\n",
    "    restricted_medians = []\n",
    "    for j in range(2, len(data) - 1):\n",
    "        splits = splitList(data, j)\n",
    "        split_deviations = [np.sum(findRestrictedMedian(split, candidates)[1:]) for split in splits[0]]\n",
    "        restricted_medians.append(np.array(split_deviations)@np.array(splits[1]))\n",
    "    return (np.argmin(restricted_medians)+1, np.min(restricted_medians))\n",
    "\n",
    "def paddedPartition(list, window):\n",
    "    a = [list[:window] for _ in range(round((window-1)/2))]\n",
    "    b = [list[j:window+j] for j in range(len(list)-window+1)]\n",
    "    c = [list[:window] for _ in range(round((window-1)/2))]\n",
    "    return np.concatenate([a,b,c])\n",
    "\n",
    "def copyNumberFilter(data, candidates, window=101):\n",
    "    return [findRestrictedMedian(part, candidates) for part in paddedPartition(data, window)]\n",
    "\n",
    "def copyNumberVariance(data, candidates, window=101):\n",
    "    restricted_medians = [findRestrictedMedian(part, candidates) for part in paddedPartition(data, window)]\n",
    "    return np.mean([z[0]*z[1]/z[2] for z in restricted_medians])\n",
    "\n",
    "def listCandidates(f, maxPloidy):\n",
    "    return 2*(1-f)+f*np.arange(1, maxPloidy+1)\n",
    "\n",
    "def computeChangePoints(profile):\n",
    "    return np.arange(len(profile)-1)[np.abs(np.diff(profile)) > 0]\n",
    "\n",
    "def refineChangePoint(data, pt, candidates, window=250, replicates=100):\n",
    "    w = round((window-1)/2)\n",
    "    split = findSplit(data[pt-w:pt+w+1], candidates)\n",
    "    a = pt-w-1+split[0]\n",
    "    replicates = [findSplit(rng.choice(data[pt-w:pt+w+1], 2*w+1), candidates)[-1] for _ in range(replicates)]\n",
    "    return (a, (replicates > split[-1]).sum() / len(replicates))\n",
    "\n",
    "def refineChangePoints(data, pts, candidates, window=250, replicates=100):\n",
    "    extended_pts = np.concatenate(([0], pts, [len(rPos)-1]))\n",
    "    parts = [extended_pts[j:j+3] for j in range(len(extended_pts)-2)]\n",
    "    ws = [ np.min(np.append(np.diff(part)/2, window)) for part in parts ]\n",
    "    res = []\n",
    "    for pt, w in zip(pts, ws):\n",
    "        if w <= 3.5:\n",
    "            res.append((pt, 0))\n",
    "        else:\n",
    "            res.append(refineChangePoint(data, pt, candidates, w, replicates))\n",
    "    return res\n",
    "\n",
    "def refineProfile(profile, pts):\n",
    "    extended_pts = np.concatenate(([0], pts, [len(rPos)]))\n",
    "    refined_segments = []\n",
    "    for j in range(len(extended_pts)-1):\n",
    "        a = int(extended_pts[j])\n",
    "        b = int(extended_pts[j+1])\n",
    "        values, counts = np.unique(profile[a:b+1], return_counts=True)\n",
    "        index = np.argmax(counts)\n",
    "        refined_segments = np.append(refined_segments, [values[index] for _ in range(b-a)])\n",
    "    return refined_segments\n",
    "\n",
    "def flattenExcursions(profile, threshold=200):\n",
    "    pts = computeChangePoints(profile)\n",
    "    partitioned_profile = partitionByIndex(profile, pts)\n",
    "    pos = np.array([ (len(part) <= threshold) for part in partitioned_profile])\n",
    "    flattened_profile = []\n",
    "    for j, part in enumerate(partitioned_profile):\n",
    "        n = len(part)\n",
    "        if (n<=threshold) & (0 < j < len(partitioned_profile)-1):\n",
    "            flattened_profile.append(np.concatenate(([partitioned_profile[j-1][0] for _ in range(round(n/2))], [partitioned_profile[j+1][0] for _ in range(round((n+1)/2)) ])))\n",
    "        else:\n",
    "            flattened_profile.append(partitioned_profile[j])\n",
    "    return np.concatenate(flattened_profile)\n",
    "\n",
    "def rankExcursions(data, refinedProfile, changePoints):\n",
    "    partitioned_profile = partitionByIndex(refinedProfile, changePoints[:,0].astype(int))\n",
    "    partitioned_data = partitionByIndex(data, changePoints[:,0].astype(int))\n",
    "    H0 = data[(1.99 <= refinedProfile) & (refinedProfile <= 2.01)]\n",
    "    extended_change_points = np.concatenate([[0.5], changePoints[:,1], [0.5]])\n",
    "    intervalP = [(extended_change_points[j] + extended_change_points[j+1]) for j in range(len(extended_change_points)-1)]\n",
    "    def extractBulk(list):\n",
    "        n = np.max([round(0.2*len(list)),1])\n",
    "        return list[n:-n]\n",
    "    def findClosestDiploidBlock(blocks, k):\n",
    "        a, b = np.nonzero(blocks < k)[0], np.nonzero(blocks > k)[0]\n",
    "        if len(a)>0:\n",
    "            if len(b)>0:\n",
    "                return [blocks[a[-1]], blocks[b[0]]]\n",
    "            else:\n",
    "                return [blocks[a[-1]]]\n",
    "        elif len(b)>0:\n",
    "            return [blocks[b[0]]]\n",
    "        else:\n",
    "            return []\n",
    "    block_ploidy = np.array([part[0] for part in partitioned_profile])\n",
    "    diploidBlocks = np.nonzero((block_ploidy <= 2.01) & (1.99 <= block_ploidy))[0]\n",
    "    stat_vals_1 = []\n",
    "    for part_profile, part_data, j in zip(partitioned_profile, partitioned_data, range(len(partitioned_profile))):\n",
    "        a = np.mean((excursionLengthsH0 <= len(part_profile)))\n",
    "        sample_data = np.concatenate([partitioned_data[j] for j in findClosestDiploidBlock(diploidBlocks, j)])\n",
    "        sampled_data = rng.choice(sample_data, size=(100, len(extractBulk(part_profile))))\n",
    "        dataH0 = [medianDeviation(sampled_point) for sampled_point in sampled_data]\n",
    "        threshold = 2 / part_profile[0] * medianDeviation(extractBulk(part_data))\n",
    "        b = np.mean(dataH0 > threshold)\n",
    "        stat_vals_1.append(np.max([a,b])**2)\n",
    "    def uniformSumCDF(x):\n",
    "        return np.piecewise(x, [x < 0, (0 <= x) & (x < 1), (1 <= x) & (x < 2), 2 <= x], [0, lambda x: x**2/2, lambda x: 1-0.5*(2-x)**2, 1])\n",
    "    stat_vals_2 = uniformSumCDF(np.array([interval.sum() for interval in intervalP]))\n",
    "    stat_vals = stat_vals_1 + stat_vals_2\n",
    "    return uniformSumCDF(stat_vals)\n",
    "\n",
    "def medianFilter(data, window):\n",
    "    return np.array([np.median(data[j:j+window]) for j in range(len(data)-window+1)])\n",
    "\n",
    "def extractExcursions(profile, points):\n",
    "    extended_points = np.concatenate([[0], points, [len(rPos)]])\n",
    "    excursion_indices = [[extended_points[j], extended_points[j+1]] for j in range(len(extended_points)-1)]\n",
    "    excursions = partitionByIndex(profile, points)\n",
    "    excursion_means = np.array([np.mean(excursion) for excursion in excursions])\n",
    "    return np.arange(len(excursion_indices))[np.abs(excursion_means - 2) > 0.001]\n",
    "\n",
    "def excursionLengths(profile, points):\n",
    "    pos = extractExcursions(profile, points)\n",
    "    return [len(partitionByIndex(profile, points)[j]) for j in pos]\n",
    "\n",
    "def benjaminiHochberg(pvalues, alpha):\n",
    "    ps = np.sort(1-pvalues) - alpha / len(pvalues) * np.arange(1, len(pvalues)+1)\n",
    "    indices = np.nonzero(ps > 0)[0]\n",
    "    if len(indices) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1+indices[0]\n",
    "\n",
    "def confidenceInterval(data, profile, f, resample_size=100):\n",
    "    pts = computeChangePoints(profile)\n",
    "    partitioned_profile = partitionByIndex(profile, pts)\n",
    "    partitioned_data = partitionByIndex(data, pts)\n",
    "    pos = extractExcursions(profile, pts)\n",
    "    partitioned_profile_pos = [partitioned_profile[j] for j in pos]\n",
    "    excursion_ploidy = np.abs([ 2 - (np.unique(profile_part)[0] - 2*(1-f)) / 2 for profile_part in partitioned_profile_pos ])\n",
    "    excursion_data = [partitioned_data[j] for j in pos]\n",
    "    excursion_lengths = np.array([len(profile_part) for profile_part in partitioned_profile_pos])\n",
    "    resample_sizes = resample_size * excursion_lengths / np.sum(excursion_lengths)\n",
    "    fluctuations = []\n",
    "    for data_point, size, ploidy in zip(excursion_data, resample_sizes, excursion_ploidy):\n",
    "        fluctuations = np.append(fluctuations, [np.abs(np.median(rng.choice(data_point, len(data_point))) - 2) / ploidy for _ in range(int(size))])\n",
    "    return 2*medianDeviation(fluctuations)\n",
    "\n",
    "def findThreshold(pvalues, alpha):\n",
    "    delta = 1\n",
    "    iter = 1\n",
    "    sorted_ps = np.sort(pvalues)[::-1]\n",
    "    while (delta>0) & (iter<np.min([1000, len(pvalues)-1])):\n",
    "        delta = benjaminiHochberg(sorted_ps[:iter+1], alpha) - benjaminiHochberg(sorted_ps[:iter], alpha)\n",
    "        iter+=1\n",
    "    return iter\n",
    "\n",
    "def estimate_proportion_ploidy(rowsums, maxPloidy):\n",
    "    data = rowsums\n",
    "    y = int(corrchromlocations[10])\n",
    "    smoothed_data = 2*medianFilter(data[:y], 100) / findDiploidPeak(medianFilter(data[:y], 100))\n",
    "    sigma = []\n",
    "    for j in tqdm(np.arange(0,1.01,0.01)):\n",
    "        sigma.append(copyNumberVariance(smoothed_data, 2*(1-j) + j*np.arange(1, maxPloidy+1), 251))\n",
    "    f = np.arange(0, 1.01, 0.01)[np.argmin(sigma)]\n",
    "    pi0 = np.array(copyNumberFilter(rowsums, 2*(1-f) + f*np.arange(1, maxPloidy+1), 501))\n",
    "    xs = np.array(refineChangePoints(data, computeChangePoints(pi0[:,0]), listCandidates(f, maxPloidy), 250))\n",
    "    pi1 = refineProfile(pi0[:,0], xs[:,0])\n",
    "    ps = rankExcursions(data, pi1, xs)\n",
    "    return pi1, f, ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eef125-efce-4ae9-a6c4-fc6509033b57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Off-diagonal detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aee3a4-dfce-4d13-b5db-5b026e6eeac0",
   "metadata": {},
   "source": [
    "Function definitions detecting fusion events of type (a) and (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1538,
   "id": "01a32ecf-b64f-404f-ae1f-c394ed968adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundToChromosome(x,y):\n",
    "    chiRange = [xToChi(z) for z in range(x,y+1)]\n",
    "    chis, counts = np.unique(chiRange, return_counts=True)\n",
    "    chi = int(np.median(chiRange))\n",
    "    if np.max(counts / np.sum(counts)) < 0.6:\n",
    "        acs = []\n",
    "        for j in chis:\n",
    "            a = np.max([x, corrchromlocations[j-1]])\n",
    "            b = np.min([y, corrchromlocations[j] - 1])\n",
    "            if corrchromlocations[xToChi(b)]-1-b < 50:\n",
    "                c = corrchromlocations[xToChi(b)] - 1\n",
    "            else:\n",
    "                c = b\n",
    "            acs.append([a,c])\n",
    "        acs = np.array(acs, dtype=int)\n",
    "        acs = acs[(acs[:,1] - acs[:,0]) > 200]\n",
    "        return acs\n",
    "    else:\n",
    "        chi_nearest_range = np.arange(corrchromlocations[chi-1], corrchromlocations[chi]-1)\n",
    "        nearest_x = chi_nearest_range[np.abs(chi_nearest_range - x).argmin()]\n",
    "        nearest_y = chi_nearest_range[np.abs(chi_nearest_range - y).argmin()]\n",
    "        if corrchromlocations[xToChi(nearest_x)]-1-nearest_x < 50:\n",
    "            a = corrchromlocations[xToChi(nearest_x)]-1\n",
    "        else:\n",
    "            a = nearest_x\n",
    "        if corrchromlocations[xToChi(nearest_y)]-1-nearest_x < 50:\n",
    "            b = corrchromlocations[xToChi(nearest_y)]-1\n",
    "        else:\n",
    "            b = nearest_y\n",
    "        return np.array([a, b], dtype=int)\n",
    "\n",
    "def scanBlock1(block, ws):\n",
    "    ws_prod = np.product(ws)\n",
    "    return sp_signal.oaconvolve(np.ones(ws), block.toarray(), mode='valid') / ws_prod\n",
    "\n",
    "def scanBlock2(mat, chi1, chi2, ws):\n",
    "    a = corrchromlocations[chi1-1]\n",
    "    b = corrchromlocations[chi1]\n",
    "    c = corrchromlocations[chi2-1]\n",
    "    d = corrchromlocations[chi2]\n",
    "    ws_prod = np.product(ws)\n",
    "    return sp_signal.oaconvolve(np.ones(ws), mat.toarray()[int(a):int(b), int(c):int(d)], mode='valid') / ws_prod\n",
    "\n",
    "def computeIntensitySquare(mat, x, y, ws):\n",
    "    a = np.max([x-ws[0]+1, 0])\n",
    "    b = np.min([x+ws[0]+1, len(rPos)])\n",
    "    c = np.max([y-ws[1]+1, 0])\n",
    "    d = np.min([y+ws[1]+1, len(rPos)])\n",
    "    mat11 = mat[a:x+1, c:y+1]\n",
    "    mat12 = mat[a:x+1, y+1:d]\n",
    "    mat21 = mat[x+1:b, c:y+1]\n",
    "    mat22 = mat[x+1:b, y+1:d]\n",
    "    return np.array([[mat11.mean(), mat12.mean()], [mat21.mean(), mat22.mean()]])\n",
    "\n",
    "def detectPattern(intensitySquare):\n",
    "    normalization_constant = intensitySquare.sum()\n",
    "    if normalization_constant <= 10**(-8):\n",
    "        return 0.\n",
    "    else:\n",
    "        normalized_densities = np.concatenate(intensitySquare) / normalization_constant\n",
    "    return np.max(normalized_densities)\n",
    "\n",
    "def rowPattern(mat, x, ws):\n",
    "    chi = xToChi(x)\n",
    "    n = len(rPos)\n",
    "    w = ws[-1]\n",
    "    y_indices = np.concatenate([np.arange(0, corrchromlocations[chi-1]), np.arange(corrchromlocations[chi], n)])\n",
    "    r_block = mat[x-ws[0]+1:x+1, y_indices]\n",
    "    r = np.concatenate(scanBlock1(r_block, ws))\n",
    "    s_block = mat[x+1:x+ws[0]+1, y_indices]\n",
    "    s = np.concatenate(scanBlock1(s_block, ws))\n",
    "    detected_patterns = []\n",
    "    for j in range(len(r)-w):\n",
    "        pattern_mat = np.array([[r[j], r[j+w]], [s[j], s[j+w]]])\n",
    "        detected_patterns.append(detectPattern(pattern_mat))\n",
    "    return np.array(detected_patterns)\n",
    "\n",
    "def testTreeStructure(mat, x, y, w, resamples=100):\n",
    "    submatrix = mat[np.max([x-w[0],0]):np.min([x+w[0]+1, len(rPos)]), np.max([y-w[1],0]):np.min([y+w[1]+1, len(rPos)])]\n",
    "    rowMarginals = submatrix.mean(axis=1)\n",
    "    columnMarginals = submatrix.mean(axis=0)\n",
    "    rowStatistic = np.mean([np.var(rowMarginals[:w[0]]) * w[0] / (w[0]-1), np.var(rowMarginals[w[0]+1:]) * len(rowMarginals[w[0]+1:]) / (len(rowMarginals[w[0]+1:]) - 1)])\n",
    "    columnStatistic = np.mean([np.var(columnMarginals[:w[0]]) * w[0] / (w[0]-1), np.var(columnMarginals[w[0]+1:]) * len(columnMarginals[w[0]+1:]) / (len(columnMarginals[w[0]+1:]) - 1)])\n",
    "    resampledRowStatistic = []\n",
    "    resampledColumnStatistic = []\n",
    "    for _ in range(resamples):\n",
    "        permutedRows = rng.permutation(rowMarginals)\n",
    "        permutedColumns = rng.permutation(columnMarginals)\n",
    "        resampledRowStatistic.append(np.mean([np.var(permutedRows[:w[0]]) * w[0] / (w[0]-1), \n",
    "                                              np.var(permutedRows[w[0]+1:]) * len(permutedRows[w[0]+1:]) / (len(permutedRows[w[0]+1:]) - 1)]))\n",
    "        resampledColumnStatistic.append(np.mean([np.var(permutedColumns[:w[0]]) * w[0] / (w[0]-1), \n",
    "                                                 np.var(permutedColumns[w[0]+1:]) * len(permutedColumns[w[0]+1:]) / (len(permutedColumns[w[0]+1:]) - 1)]))\n",
    "    return np.mean([np.mean(resampledRowStatistic > rowStatistic), np.mean(resampledColumnStatistic > columnStatistic)])\n",
    "\n",
    "def extractButterflyCandidate(mat, chi1, chi2, ws, componentDepth=10):\n",
    "    blockScan = scanBlock2(mat, chi1, chi2, ws)\n",
    "    components = sp_image.label(np.where(blockScan > np.median(blockScan), blockScan, 0))[0]\n",
    "    component_groups, component_counts = np.unique(components, return_counts=True)\n",
    "    size_threshold = np.sort(component_counts)[-componentDepth]\n",
    "    large_groups = component_groups[component_counts >= size_threshold]\n",
    "    large_group_sizes = component_counts[component_counts >= size_threshold]\n",
    "    group_maxs = [[np.unravel_index(np.where(components == group, blockScan, 0).argmax(), blockScan.shape), round(np.sqrt(size/2))] \n",
    "                  for group, size in zip(large_groups, large_group_sizes)]\n",
    "    return group_maxs\n",
    "\n",
    "def pickLargeCandidate(mat, x, y, ws):\n",
    "    localMat = mat[np.max([x-ws[0], 1]):np.min([x+ws[0], len(rPos)]), np.max([y-ws[1], 1]):np.min([y+ws[1], len(rPos)])]\n",
    "    return np.array(np.unravel_index(localMat.argmax(), localMat.shape)) + np.array([1+x,1+y]) - np.array([1+ws[0], 1+ws[1]])\n",
    "\n",
    "def butterflySummary(mat, x, y, ws):\n",
    "    intensitySquare = computeIntensitySquare(mat, x, y, ws)\n",
    "    if intensitySquare.sum() <= 0:\n",
    "        intensitySquare = np.zeros([2,2])\n",
    "    else:\n",
    "        intensitySquare = intensitySquare / intensitySquare.sum()\n",
    "    return -np.linalg.det(intensitySquare)\n",
    "\n",
    "def findButterflySummary(mat, x, y, ws, symmetry=0):\n",
    "    a = np.max([0, x-ws[0]])\n",
    "    b = np.min([len(rPos), x+ws[0]+1])\n",
    "    c = np.max([0, y-ws[1]])\n",
    "    d = np.min([len(rPos), y+ws[1]+1])\n",
    "    localMat = np.array([[butterflySummary(mat, j, k, [50, 50]) for k in np.arange(c,d)] for j in np.arange(a,b)])\n",
    "    if symmetry == 0:\n",
    "        argmax = np.unravel_index(localMat.argmax(), localMat.shape)\n",
    "        return [np.array(argmax) + np.array([x, y]) - np.array(ws), localMat[argmax]]\n",
    "    else:\n",
    "        argmin = np.unravel_index(localMat.argmin(), localMat.shape)\n",
    "        return [np.array(argmin) + np.array([x, y]) - np.array(ws), localMat[argmin]]\n",
    "    return localMat\n",
    "\n",
    "def treeStatistic(mat, x, y, w):\n",
    "    a11 = mat[np.max([0, x-w-1]):x, np.max([0, y-w]):y+1][::-1]\n",
    "    a12 = np.transpose(mat[np.max([0, x-w-1]):x, y+1:np.min([len(rPos), y+w+2])])\n",
    "    a21 = mat[x:np.min([len(rPos), x+w+1]), np.max([0, y-w]):y+1]\n",
    "    a22 = np.transpose(mat[x:np.min([len(rPos), x+w+1]), y+1:np.min([len(rPos), y+2+w])][::-1])\n",
    "    quad_stats = []\n",
    "    for quad in [a11, a12, a21, a22]:\n",
    "        w_steps, diags = np.transpose(np.array([[j, np.diagonal(quad.toarray(), j).mean()] for j in np.arange(-w, w+1)]))\n",
    "        if np.var(diags) <= 0:\n",
    "            quad_stats.append(0)\n",
    "        else:\n",
    "            quad_stats.append(kendalltau(w_steps, diags).statistic)\n",
    "    tree_stats = (1 + np.array(quad_stats).reshape([2,2]))/2\n",
    "    return -np.linalg.det(tree_stats)\n",
    "\n",
    "def detectButterfly(intensitySquare, symmetry=0):\n",
    "    partition_sum = intensitySquare.sum()\n",
    "    if partition_sum == 0:\n",
    "        normalized_square = [[0,0], [0,0]]\n",
    "    else:\n",
    "        normalized_square = intensitySquare / partition_sum\n",
    "    if symmetry == 0:\n",
    "        return [normalized_square[1,0], normalized_square[0,1]]\n",
    "    else:\n",
    "        return [normalized_square[0,0], normalized_square[1,1]]\n",
    "\n",
    "def testButterflyStructure(mat, x, y, w, resamples=100):\n",
    "    submatrix = mat[np.max([0, x-w[0]]):np.min([len(rPos), x+w[0]+1]), np.max([0, y-w[1]]):np.min([len(rPos), y+w[1]+1])].toarray()\n",
    "    def formSubmatrices(matrix):\n",
    "        return matrix[:w[0], :w[1]], matrix[:w[0], w[1]+1:], matrix[w[0]+1:, :w[1]], matrix[w[0]+1:, w[1]+1:]\n",
    "    treeStat = np.mean([matrix.var() * matrix.size / (matrix.size - 1) for matrix in formSubmatrices(submatrix)])\n",
    "    permutedTreeStat = []\n",
    "    for _ in range(resamples):\n",
    "        n, m = submatrix.shape\n",
    "        permutedSubmatrix = submatrix[rng.permutation(n), :][:, rng.permutation(m)]\n",
    "        permutedTreeStat.append(np.mean([matrix.var() * matrix.size / (matrix.size - 1) for matrix in formSubmatrices(permutedSubmatrix)]))\n",
    "    return (permutedTreeStat > treeStat).mean()\n",
    "\n",
    "def testButterflyCandidate(mat, point, ws, diagonalw=10, resamples=10, symmetry=0):\n",
    "    largeCandidate = pickLargeCandidate(mat, *point, ws)\n",
    "    chis = np.sort([xToChi(point[0]), xToChi(point[1])])\n",
    "    if symmetry == 0:\n",
    "        shift = np.array([1, 0])\n",
    "        intensity_indices = np.array([[0,0], [0,-1], [1,-1], [1,0], [-1,1], [-1,2], [-2,2], [-2,1]])\n",
    "    else:\n",
    "        shift = np.array([0, 0])\n",
    "        intensity_indices = np.array([[0,0], [0,-1], [-1,-1], [-1,0], [1,1], [1,2], [2,2], [2,1]])\n",
    "    candidate = findButterflySummary(hic, *largeCandidate, np.round(np.array(ws) / 2).astype(int), symmetry)\n",
    "    candidate = [candidate[0] + shift, candidate[1]]\n",
    "    structure = testButterflyStructure(mat, *candidate[0], ws, resamples)\n",
    "    treeStat = treeStatistic(mat, *candidate[0], diagonalw)\n",
    "    blockScan = scanBlock2(mat, *chis, [3,3])\n",
    "    blockScan = blockScan[blockScan > 10**(-10)]\n",
    "    intensity_ratio_a = np.mean([mat[*index] for index in (candidate[0] + intensity_indices)]) - np.mean(blockScan)\n",
    "    intensity_ratio_b = np.std(blockScan) * np.sqrt(len(blockScan) / (len(blockScan) - 1))\n",
    "    intensity_ratio = intensity_ratio_a / intensity_ratio_b\n",
    "    detected_butterfly = detectButterfly(computeIntensitySquare(mat, *candidate[0], ws), symmetry)\n",
    "    return [candidate[0][0], candidate[0][1], candidate[1], treeStat, structure, intensity_ratio, detected_butterfly[0], detected_butterfly[1]]\n",
    "\n",
    "def calibrateButterflyTests(testMat, ratioThreshold=5, symmetry=0):\n",
    "    flatten_test_matrix = testMat[:,:,[2,3]].reshape(21*21,2)\n",
    "    flatten_test_matrix = flatten_test_matrix[(np.abs(flatten_test_matrix).mean(axis=1) > 0) & (flatten_test_matrix.mean(axis=1) != np.nan)]\n",
    "    thetas = np.transpose([flatten_test_matrix.mean(axis=0), flatten_test_matrix.std(axis=0) * np.sqrt(len(flatten_test_matrix) / (len(flatten_test_matrix)-1))])\n",
    "    calibrated_values = np.zeros((testMat.shape)[:2])\n",
    "    if symmetry == 0:\n",
    "        for j in range(len(testMat)):\n",
    "            for k in range(len(testMat)):\n",
    "                element = testMat[j, k]\n",
    "                if (np.array_equal(element, np.zeros(8))) or (element[5] < ratioThreshold):\n",
    "                    calibrated_values[j,k] = 0\n",
    "                else:\n",
    "                    a = 1-sp_stats.norm(loc=thetas[0,0], scale=thetas[0,1]).cdf(element[2])\n",
    "                    b = 1-sp_stats.norm(loc=thetas[1,0], scale=thetas[1,1]).cdf(element[3])\n",
    "                    c = element[4]\n",
    "                    print(element)\n",
    "                    calibrated_values[j,k] = np.min([a, b, c])\n",
    "    return calibrated_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3118f9-399c-4abf-a9bb-f7aaa1dde743",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae7f46-47ba-4179-98b5-b1ca732b6a64",
   "metadata": {},
   "source": [
    "Each of the following sections performs parts of the analysis described in the main paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41593a04-7509-4c7e-a2a5-af2aed43832a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load Hi-C Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c9174-6b4f-44d8-b0b1-ce6d59a3d2f0",
   "metadata": {},
   "source": [
    "Replace \"hi-c_matrix\" with filename of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "d70ac4a9-acf1-4f45-b257-c2930d620167",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_indices, col_indices, hic_vals = np.transpose(np.loadtxt('hi-c_matrix'))\n",
    "relevant_indices = (row_indices < 57509 - 1) & (col_indices < 57509 - 1)\n",
    "hic = sp_sparse.coo_array((hic_vals[relevant_indices], (row_indices[relevant_indices].astype(int)-1, col_indices[relevant_indices].astype(int)-1)), (57509, 57509))\n",
    "hic = sp_sparse.csr_array(hic)\n",
    "del row_indices, col_indices, hic_vals, relevant_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c034506-9d4c-49f7-b5ee-57162afed231",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Copy Number & Mixture Proportion Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5657dda-0339-4701-b9df-41c8893b0831",
   "metadata": {},
   "source": [
    "Estimation of effective copy number profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "6e22137e-cc93-4a7b-afa7-a29ff4e3f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment the respective lines, if the experimental protocol (Fix-C or Hi-C) of the contact map in question is known.\n",
    "\n",
    "rowsums = covariateCorrection1(attachCovariates(hic.diagonal()))\n",
    "# rowsums = dataCcorrector(hic.diagonal(), 'FixC')\n",
    "# rowsums = dataCcorrector(hic.diagonal(), 'HiC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "261a01e5-7bdf-4656-8882-d9aa9d988e3c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 101/101 [05:30<00:00,  3.27s/it]\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "pi1, f, ps = estimate_proportion_ploidy(rowsums, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fcbe8c-7a36-4f05-8f3a-c44cbfa0324b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Off-diagonal inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ce2b1-2c51-4a12-bed0-426c4b876c43",
   "metadata": {},
   "source": [
    "Filter Hi-C matrix by criteria specified in covariate correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1591,
   "id": "47d71851-0a7f-4be5-aa90-59f13eed70d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hic = hic[rPos,:][:,rPos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570aa21c-c15b-4922-9afe-53a119b32225",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Type (a) events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1593,
   "id": "a792ff22-edd5-4fdb-8e43-2e4253360b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract change point candidates\n",
    "\n",
    "candidates = np.abs(np.diff(medianFilter(pi1, 50))).nonzero()[0]\n",
    "candidates = candidates[candidates < corrchromlocations[-1]]\n",
    "aCandidates = []\n",
    "extended_candidates = np.concatenate([[0], candidates, [len(rPos)]])\n",
    "for j in range(1, len(extended_candidates)-1):\n",
    "    if np.max([extended_candidates[j] - extended_candidates[j-1], extended_candidates[j+1] - extended_candidates[j]]) > 100:\n",
    "        aCandidates.append(extended_candidates[j])\n",
    "del candidates, extended_candidates, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "id": "b80296a0-2ba5-4377-aa0e-8a0d0c780bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate row-wise null distributions for extracted change-point candidates\n",
    "\n",
    "mat = filtered_hic + filtered_hic.transpose() - sp_sparse.diags([filtered_hic.diagonal()], [0])\n",
    "ws = [300, 300]\n",
    "testPoints = np.concatenate([aCandidates, acrox]).astype(int)\n",
    "aRow = []\n",
    "for testPoint in testPoints:\n",
    "    if testPoint < np.min(ws):\n",
    "        aRow.append(rowPattern(mat, testPoint, [testPoint, testPoint]))\n",
    "    else:\n",
    "        aRow.append(rowPattern(mat, testPoint, ws))\n",
    "del mat, ws, testPoints, testPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "id": "1e547fc1-0d1c-4121-9b9a-518fe973011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for each pair of change-point candidates\n",
    "\n",
    "mat = filtered_hic + filtered_hic.transpose() - sp_sparse.diags([filtered_hic.diagonal()], [0])\n",
    "ws = [300, 300]\n",
    "testPoints = np.concatenate([aCandidates, acrox]).astype(int)\n",
    "aSummary = np.nan * np.ones([len(testPoints), len(testPoints), 3])\n",
    "for k in range(len(testPoints)):\n",
    "    for j in range(len(testPoints)):\n",
    "        x = testPoints[k]\n",
    "        y = testPoints[j]\n",
    "        rowSamples = aRow[k]\n",
    "        columnSamples = aRow[j]\n",
    "        target = detectPattern(computeIntensitySquare(mat, x, y, ws))\n",
    "        if (x>=y) or (xToChi(x) >= xToChi(y)) or (np.max([len(testPoints) - k, len(testPoints) - j]) <= 3):\n",
    "            aSummary[k, j] = [0, 0, 0]\n",
    "        else: \n",
    "            aSummary[k, j] = [(rowSamples <= target).mean(), (columnSamples <= target).mean(), testTreeStructure(mat, x, y, ws, 100) ]\n",
    "del mat, ws, testPoints, k ,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "id": "4d6b1d75-7bc8-43d5-8f46-1ade57bbfe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate summary statistics into well-calibrated p-values\n",
    "\n",
    "testPoints = np.concatenate([aCandidates, acrox]).astype(int)\n",
    "aP = np.nan * np.ones([len(testPoints), len(testPoints)])\n",
    "for k in range(len(testPoints)):\n",
    "    for j in range(len(testPoints)):\n",
    "        if (k>=j) or (xToChi(testPoints[k]) >= xToChi(testPoints[j])) or (np.abs(testPoints[k] - testPoints[j])<2000):\n",
    "            aP[k,j] = 0\n",
    "        else:\n",
    "            aP[k,j] = np.min(aSummary[k,j])\n",
    "del testPoints, k, j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761dc15-47da-4b9f-affd-b4efa4bafed4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Type (b) events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "id": "5cdd443a-b029-4fbf-92fd-ef1a894d668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract candidate points\n",
    "\n",
    "bCandidates = np.nan * np.ones([21, 19 + 2, 3])\n",
    "for chi1 in range(1, 22):\n",
    "    for chi2 in range(1, 22):\n",
    "        if chi1 < chi2:\n",
    "            [(pt1, pt2), size] = extractButterflyCandidate(filtered_hic, chi1, chi2, [50,50], 1)[0]\n",
    "            bCandidates[chi1-1, chi2-1] = np.array([pt1, pt2, size])\n",
    "del chi1, chi2, pt1, pt2, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1499,
   "id": "e9ee2180-cf17-417a-ae55-8270ccaeaa96",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7b/x_r4cg2j41xg8t6tyrdmnhtw0000gq/T/ipykernel_41107/1393203806.py:162: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  treeStat = np.mean([matrix.var() * matrix.size / (matrix.size - 1) for matrix in formSubmatrices(submatrix)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_methods.py:261: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/var/folders/7b/x_r4cg2j41xg8t6tyrdmnhtw0000gq/T/ipykernel_41107/1393203806.py:167: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  permutedTreeStat.append(np.mean([matrix.var() * matrix.size / (matrix.size - 1) for matrix in formSubmatrices(permutedSubmatrix)]))\n",
      "/var/folders/7b/x_r4cg2j41xg8t6tyrdmnhtw0000gq/T/ipykernel_41107/1393203806.py:139: RuntimeWarning: Mean of empty slice.\n",
      "  w_steps, diags = np.transpose(np.array([[j, np.diagonal(quad.toarray(), j).mean()] for j in np.arange(-w, w+1)]))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/linalg/linalg.py:2139: RuntimeWarning: invalid value encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/sparse/_base.py:651: RuntimeWarning: divide by zero encountered in divide\n",
      "  return self.astype(np.float_)._mul_scalar(1./other)\n"
     ]
    }
   ],
   "source": [
    "# Compute summary statistics\n",
    "\n",
    "mat = filtered_hic\n",
    "ws = [50, 50]\n",
    "bP = np.nan * np.ones([2, 21, 21, 8])\n",
    "for par in [0,1]:\n",
    "    for chi1 in range(1, 22):\n",
    "        for chi2 in range(1, 22):\n",
    "            if chi1 >= chi2:\n",
    "                bP[par, chi1-1, chi2-1] = np.zeros(8)\n",
    "            else:\n",
    "                bP[par, chi1-1, chi2-1] = testButterflyCandidate(mat, (corrchromlocations[[chi1-1, chi2-1]]+25+bCandidates[chi1-1, chi2-1, 1]).astype(int), ws, 10, 100, par)\n",
    "del mat, ws, chi1, chi2, par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1544,
   "id": "c662f198-fde9-49f9-8fca-dd38525c1c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert summaries into well-calibrated p-values\n",
    "\n",
    "calibratedbP = np.array([calibrateButterflyTests(bP[0], 5, 0), calibrateButterflyTests(bP[1], 5, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1587,
   "id": "e1ea140d-f0d1-4426-9533-47c799626c72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Report events corresponding to significant p-values\n",
    "\n",
    "candidatesa = np.concatenate([aCandidates, acrox]).astype(int)\n",
    "candidatesb = np.nan * np.ones([2, len(bP[0]), len(bP[0]), n, 5])\n",
    "for par in [0,1]:\n",
    "    for j in range(len(bP[0])):\n",
    "        for k in range(len(bP[0])):\n",
    "            candidatesb[par, j, k] = np.array([bP[par, j, k, 0], bP[par, j, k, 1], xToChi(bP[par, j, k, 0]), xToChi(bP[par, j, k, 1]), calibratedbP[par, j, k]])\n",
    "list = np.nan * np.ones([len(candidatesa), len(candidatesa), 5])\n",
    "for j in range(len(candidatesa)):\n",
    "    for k in range(len(candidatesa)):\n",
    "        list[j,k] = np.array([candidatesa[j], candidatesa[k], xToChi(candidatesa[j]), xToChi(candidatesa[k]), aP[j,k]])\n",
    "list = np.concatenate([np.concatenate(list), np.concatenate(np.concatenate(candidatesb[0])), np.concatenate(np.concatenate(candidatesb[1]))])\n",
    "list = list[list[:,-1] > 0]\n",
    "list = list[list[:,-1].argsort()][::-1]\n",
    "threshold = findThreshold(list[:,-1], 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
